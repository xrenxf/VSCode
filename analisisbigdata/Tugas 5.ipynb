{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yamDI1wAOaO"
      },
      "source": [
        "# Introduction to Supervised Learning for Big Data\n",
        "\n",
        "Contoh berikut adalah implementasi supervised learning dalam konteks big data. Terutama, fokus pada kecepatan komputasi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bRZ8QhlAOaP"
      },
      "source": [
        "Sebelum dimulai, berikut adalah metode untuk mengukur waktu eksekusi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "scrolled": true,
        "id": "BzWmHuazAOaP"
      },
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aKBFumOQAOaP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be3321c5-3615-49f0-cbb5-830217b83378"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time: 3.0028927326202393 seconds\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "time.sleep(3)  # an operation you want to evaluate\n",
        "elapsed_time = time.time() - start_time\n",
        "print('Elapsed time: {} seconds'.format(elapsed_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlLx1KTwAOaP"
      },
      "source": [
        "Menggunakan cara ini, mari kita ukur berapa lama waktu eksekusi sistem linear."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRDm3jrmAOaQ"
      },
      "source": [
        "Pertama, terdapat matrix $X\\in \\mathbb{R}^{N\\times d}$ dan $Y \\in \\mathbb{R}^{N\\times 1}$ untuk beberapa integer pofitif $N$ dan $d < N$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "o8YADYB9AOaQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-nZWJu72AOaQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc17315d-144e-4937-d33d-18aae702302d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.0092584  -0.05155933 -0.63938133 ... -0.81276714  0.18985544\n",
            "   0.51100434]\n",
            " [-1.07290968  0.79838761 -0.30924359 ... -0.1107372  -0.47734671\n",
            "   2.03921444]\n",
            " [ 0.16161299 -1.53734186 -0.06170273 ... -1.31733631 -0.17028783\n",
            "  -1.4085652 ]\n",
            " ...\n",
            " [ 1.12600886 -0.19601986 -1.28124697 ...  0.71852638  1.57901976\n",
            "  -0.46438197]\n",
            " [ 0.85178491  0.42955739 -0.17704912 ...  2.42262787  0.84601429\n",
            "  -0.50677911]\n",
            " [ 1.59915763 -0.76716477 -0.24586984 ...  0.39212999  0.72757443\n",
            "  -1.54608596]]\n",
            "[[ 0.49162459]\n",
            " [ 0.28031612]\n",
            " [-0.89856809]\n",
            " ...\n",
            " [-0.21447013]\n",
            " [ 0.14521757]\n",
            " [ 0.45407945]]\n"
          ]
        }
      ],
      "source": [
        "N = 10000\n",
        "d = 500\n",
        "X = np.random.normal(loc=0, scale=1, size=[N, d])\n",
        "Y = np.random.normal(loc=0, scale=1, size=[N, 1])\n",
        "\n",
        "print(X)\n",
        "print(Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR6evjbyAOaR"
      },
      "source": [
        "Untuk persamaan sistem linear $Y = XA$, solusi kuadrat terkecil untuk sistem ini dikenal sebagai:\n",
        "\n",
        "\\begin{equation*}\n",
        "A = ((X^\\top X)^{-1}X^\\top)Y\n",
        "\\end{equation*}\n",
        "\n",
        "Untuk menghitung persamaan in, pendekatan yang sederhana adalah (1) hitung $X^\\top X$ first, (2) hitung invers $(X^\\top X)^{-1}$, (3) kalikan $X^\\top$ ke hasil, dan terakhir (4) kalikan $Y$. Berikut ini adalah analisis tentang berapa lama waktu komputasi yang diperlukan untuk setiap langkah."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QjyoK9euAOaR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "776a48c3-9e44-47aa-a920-15e2661301f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time for XTX: 0.4805736541748047 seconds\n",
            "Elapsed time for the inverse: 0.11841201782226562 seconds\n",
            "Elapsed time for the inverse times XT: 0.6154403686523438 seconds\n",
            "Elapsed time for the inverse times XT times Y: 0.009085416793823242 seconds\n",
            "Total: 1.2235114574432373 seconds\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "XTX = np.matmul(X.T, X)\n",
        "XTX_elapsed_time = time.time() - start_time\n",
        "print('Elapsed time for XTX: {} seconds'.format(XTX_elapsed_time))\n",
        "\n",
        "start_time = time.time()\n",
        "inv = np.linalg.inv(XTX)\n",
        "inv_elapsed_time = time.time() - start_time\n",
        "print('Elapsed time for the inverse: {} seconds'.format(inv_elapsed_time))\n",
        "\n",
        "start_time = time.time()\n",
        "invXT = np.matmul(inv, X.T)\n",
        "invXT_elapsed_time = time.time() - start_time\n",
        "print('Elapsed time for the inverse times XT: {} seconds'.format(invXT_elapsed_time))\n",
        "\n",
        "start_time = time.time()\n",
        "A = np.matmul(invXT, Y)\n",
        "A_elapsed_time = time.time() - start_time\n",
        "print('Elapsed time for the inverse times XT times Y: {} seconds'.format(A_elapsed_time))\n",
        "\n",
        "print('Total: {} seconds'.format(XTX_elapsed_time + inv_elapsed_time + invXT_elapsed_time + A_elapsed_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ekk2zuo5AOaR"
      },
      "source": [
        "Kini, trik sederhana dapat membuat perbedaan besar dalam waktu komputasi. Pertimbangkan persamaan yang sama seperti di atas, tetapi kali ini, mari kita ubah sedikit urutan komputasinya.\n",
        "\n",
        "\\begin{equation*}\n",
        "A = (X^\\top X)^{-1}(X^\\top Y)\n",
        "\\end{equation*}\n",
        "\n",
        "Yaitu, kali ini, kita akan (1) menghitung $X^\\top X$ terlebih dahulu, (2) mengambil invers $(X^\\top X)^{-1}$, (3) menghitung $X^\\top Y$, dan terakhir (4) mengalikan $(X^\\top X)^{-1}$ dan $X^\\top Y$. Langkah (1) dan (2) sama, tetapi (3) dan (4) dalam urutan yang berbeda. Mari kita lihat berapa banyak waktu yang diperlukan untuk menghitung solusi dengan strategi ini."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "scrolled": true,
        "id": "Xiqc2NR-AOaR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffe63cbc-1a73-4eb4-9c45-f004e1710572"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time for XTX: 0.5222883224487305 seconds\n",
            "Elapsed time for the inverse: 0.0856313705444336 seconds\n",
            "Elapsed time for XTY: 0.009995460510253906 seconds\n",
            "Elapsed time for the inverse times XTY: 0.001585245132446289 seconds\n",
            "Total: 0.6195003986358643 seconds\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "XTX = np.matmul(X.T, X)\n",
        "XTX_elapsed_time = time.time() - start_time\n",
        "print('Elapsed time for XTX: {} seconds'.format(XTX_elapsed_time))\n",
        "\n",
        "start_time = time.time()\n",
        "inv = np.linalg.inv(XTX)\n",
        "inv_elapsed_time = time.time() - start_time\n",
        "print('Elapsed time for the inverse: {} seconds'.format(inv_elapsed_time))\n",
        "\n",
        "start_time = time.time()\n",
        "XTY = np.matmul(X.T, Y)\n",
        "XTY_elapsed_time = time.time() - start_time\n",
        "print('Elapsed time for XTY: {} seconds'.format(XTY_elapsed_time))\n",
        "\n",
        "start_time = time.time()\n",
        "A = np.matmul(inv, XTY)\n",
        "A_elapsed_time = time.time() - start_time\n",
        "print('Elapsed time for the inverse times XTY: {} seconds'.format(A_elapsed_time))\n",
        "\n",
        "print('Total: {} seconds'.format(XTX_elapsed_time + inv_elapsed_time + XTY_elapsed_time + A_elapsed_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Joh4wtv7AOaR"
      },
      "source": [
        "Perhatikan pengurangan waktu komputasi yang signifikan?\n",
        "\n",
        "### Tugas\n",
        "- Langkah manakah yang menunjukkan perbedaan terbesar?\n",
        "- Mengapa?\n",
        "- Tetapkan $d = 500$ tetapi cobalah untuk meningkatkan $N$ dari 10.000 menjadi 20.000, 50.000, dan 100.000. Bagaimana waktu komputasi berubah? Apakah ada pola tertentu?\n",
        "- Tetapkan $N = 10000$ tetapi tingkatkan $d$ dari 500 menjadi 1.000, 2.000, dan 5.000. Bagaimana waktu komputasi berubah? Apakah ada pola tertentu?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZXp1H-aXAOaR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09a71c29-d249-4682-dff8-d0aad4c18c74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Varying N with fixed d=500:\n",
            "Running for N=10000, d=500\n",
            "Elapsed time for XTX: 0.4745 seconds\n",
            "Elapsed time for the inverse: 0.0689 seconds\n",
            "Elapsed time for XTY: 0.0072 seconds\n",
            "Elapsed time for the inverse times XTY: 0.0003 seconds\n",
            "Total time: 0.5510 seconds\n",
            "\n",
            "Running for N=20000, d=500\n",
            "Elapsed time for XTX: 0.8097 seconds\n",
            "Elapsed time for the inverse: 0.0646 seconds\n",
            "Elapsed time for XTY: 0.0129 seconds\n",
            "Elapsed time for the inverse times XTY: 0.0003 seconds\n",
            "Total time: 0.8875 seconds\n",
            "\n",
            "Running for N=50000, d=500\n",
            "Elapsed time for XTX: 1.6377 seconds\n",
            "Elapsed time for the inverse: 0.0858 seconds\n",
            "Elapsed time for XTY: 0.0526 seconds\n",
            "Elapsed time for the inverse times XTY: 0.0075 seconds\n",
            "Total time: 1.7835 seconds\n",
            "\n",
            "Running for N=100000, d=500\n",
            "Elapsed time for XTX: 2.0007 seconds\n",
            "Elapsed time for the inverse: 0.0384 seconds\n",
            "Elapsed time for XTY: 0.0561 seconds\n",
            "Elapsed time for the inverse times XTY: 0.0023 seconds\n",
            "Total time: 2.0975 seconds\n",
            "\n",
            "\n",
            "Varying d with fixed N=10000:\n",
            "Running for N=10000, d=500\n",
            "Elapsed time for XTX: 0.1986 seconds\n",
            "Elapsed time for the inverse: 0.0419 seconds\n",
            "Elapsed time for XTY: 0.0092 seconds\n",
            "Elapsed time for the inverse times XTY: 0.0002 seconds\n",
            "Total time: 0.2499 seconds\n",
            "\n",
            "Running for N=10000, d=1000\n",
            "Elapsed time for XTX: 0.8172 seconds\n",
            "Elapsed time for the inverse: 0.2462 seconds\n",
            "Elapsed time for XTY: 0.0094 seconds\n",
            "Elapsed time for the inverse times XTY: 0.0008 seconds\n",
            "Total time: 1.0737 seconds\n",
            "\n",
            "Running for N=10000, d=2000\n",
            "Elapsed time for XTX: 1.8621 seconds\n",
            "Elapsed time for the inverse: 0.8317 seconds\n",
            "Elapsed time for XTY: 0.0123 seconds\n",
            "Elapsed time for the inverse times XTY: 0.0024 seconds\n",
            "Total time: 2.7085 seconds\n",
            "\n",
            "Running for N=10000, d=5000\n",
            "Elapsed time for XTX: 10.1149 seconds\n",
            "Elapsed time for the inverse: 18.9356 seconds\n",
            "Elapsed time for XTY: 0.0558 seconds\n",
            "Elapsed time for the inverse times XTY: 0.0337 seconds\n",
            "Total time: 29.1400 seconds\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def measure_computation_time(N, d):\n",
        "    # Generate random data for X and Y\n",
        "    X = np.random.randn(N, d)  # Random matrix X of shape (N, d)\n",
        "    Y = np.random.randn(N, 1)  # Random matrix Y of shape (N, 1)\n",
        "\n",
        "    # Measure time for each step\n",
        "    start_time = time.time()\n",
        "    XTX = np.matmul(X.T, X)\n",
        "    XTX_elapsed_time = time.time() - start_time\n",
        "    print('Elapsed time for XTX: {:.4f} seconds'.format(XTX_elapsed_time))\n",
        "\n",
        "    start_time = time.time()\n",
        "    inv = np.linalg.inv(XTX)\n",
        "    inv_elapsed_time = time.time() - start_time\n",
        "    print('Elapsed time for the inverse: {:.4f} seconds'.format(inv_elapsed_time))\n",
        "\n",
        "    start_time = time.time()\n",
        "    XTY = np.matmul(X.T, Y)\n",
        "    XTY_elapsed_time = time.time() - start_time\n",
        "    print('Elapsed time for XTY: {:.4f} seconds'.format(XTY_elapsed_time))\n",
        "\n",
        "    start_time = time.time()\n",
        "    A = np.matmul(inv, XTY)\n",
        "    A_elapsed_time = time.time() - start_time\n",
        "    print('Elapsed time for the inverse times XTY: {:.4f} seconds'.format(A_elapsed_time))\n",
        "\n",
        "    total_time = XTX_elapsed_time + inv_elapsed_time + XTY_elapsed_time + A_elapsed_time\n",
        "    print('Total time: {:.4f} seconds\\n'.format(total_time))\n",
        "    return XTX_elapsed_time, inv_elapsed_time, XTY_elapsed_time, A_elapsed_time, total_time\n",
        "\n",
        "# Part 1: Varying N (Fix d=500)\n",
        "d = 500\n",
        "Ns = [10000, 20000, 50000, 100000]\n",
        "print(\"Varying N with fixed d=500:\")\n",
        "for N in Ns:\n",
        "    print(f'Running for N={N}, d={d}')\n",
        "    measure_computation_time(N, d)\n",
        "\n",
        "# Part 2: Varying d (Fix N=10000)\n",
        "N = 10000\n",
        "ds = [500, 1000, 2000, 5000]\n",
        "print(\"\\nVarying d with fixed N=10000:\")\n",
        "for d in ds:\n",
        "    print(f'Running for N={N}, d={d}')\n",
        "    measure_computation_time(N, d)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SusZbYoAOaS"
      },
      "source": [
        "##### Jelaskan analisa sesuai pertanyaan di sini"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezKFRFXxAOaS"
      },
      "source": [
        "### Note: Advanced Profiling\n",
        "\n",
        "Mengukur waktu untuk menjalankan operasi tahap demi tahahap disebut sebagai pembuatan profil. Menggunakan perintah `time` cukup mudah, tetapi terkadang kita mungkin memerlukan beberapa metode yang lebih canggih. Misalnya, Anda mungkin telah memperhatikan bahwa waktu komputasi dari kode yang sama dapat bervariasi setiap kali Anda menjalankan kode tersebut.\n",
        "\n",
        "Salah satu cara untuk membuat profil kode Anda adalah dengan menggunakan tag `%timeit` di depan baris yang ingin Anda evaluasi. Misalnya:\n",
        "```python\n",
        "%timeit inv = np.linalg.inv(XTX)\n",
        "```\n",
        "menjalankan `inv = np.linalg.inv(XTX)` beberapa kali dan mengambil rata-rata dan deviasi standar dari waktu komputasi.\n",
        "\n",
        "Cara lain untuk melakukannya adalah dengan menggunakan tag `%prun` di depan baris. Misalnya:\n",
        "```python\n",
        "%prun inv = np.linalg.inv(XTX)\n",
        "```\n",
        "akan memberikan perincian proses yang lebih mendalam. Namun, jika Anda tidak begitu familiar dengan pemrograman komputer, `%prun` mungkin terlalu berlebihan, karena memberikan informasi yang terlalu rinci. Dalam kasus ini, Anda cukup menggunakan `%timeit` atau metode `time.time()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "scrolled": true,
        "id": "SFac52PzAOaS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48eed209-25f9-4c77-82ec-4907a2d7e692"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43.7 ms ± 2.72 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
          ]
        }
      ],
      "source": [
        "%timeit inv = np.linalg.inv(XTX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "C__pXeAWAOaS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ac43baa-1c46-4bf7-f8e1-1145f689c519"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " "
          ]
        }
      ],
      "source": [
        "%prun inv = np.linalg.inv(XTX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvK_muxDAOaS"
      },
      "source": [
        "## Distributed Optimization\n",
        "\n",
        "Meskipun ada trik komputasi di atas, mungkin masih sulit untuk menghitung pseudo-invers ($(X^\\top X)^{-1}X^\\top Y$) karena keterbatasan ruang memori, dll. Faktanya, banyak kerangka kerja analitik big data (termasuk Spark) menggunakan strategi pengoptimalan terdistribusi. Di bawah ini, kita akan melihat contoh algoritma penurunan gradien terdistribusi untuk memahami cara kerjanya."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hSDMYk9AOaS"
      },
      "source": [
        "[Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) adalah algoritma optimasi orde pertama yang populer untuk menemukan minimum lokal. Kita tidak akan membahas lebih dalam tentang apa sebenarnya algoritma gradient descent itu, tetapi berikut adalah rumus pembaruan iteratif yang digunakan oleh algoritma gradient descent:\n",
        "\n",
        "$\\theta \\longleftarrow \\theta - \\alpha \\frac{\\partial\\mathcal{L}}{\\partial \\theta}$\n",
        "\n",
        "di mana $\\theta$ adalah parameter model ($w$ dan $b$ dalam kasus regresi linier). $\\frac{\\partial\\mathcal{L}}{\\partial \\theta}$ adalah turunan orde pertama (gradien) dari lost function (kesalahan) $\\mathcal{L}$ sehubungan dengan parameter model $\\theta$.\n",
        "\n",
        "Ingat, lost function diformulasikan sebagai:\n",
        "\n",
        "$\\mathcal{L}(w, b) = \\frac{1}{N}\\sum_{i=1}^N\\|x^{(i)}w+b - y^{(i)}\\|^2$\n",
        "\n",
        "Jika kita mengembangkan simbol penjumlahan, lost function akan terlihat seperti ini:\n",
        "\n",
        "$\\mathcal{L} = \\mathcal{L}^{(1)} + \\mathcal{L}^{(2)} + \\cdots + \\mathcal{L}^{(N)}$\n",
        "\n",
        "Perhatikan $\\mathcal{L}^{(i)}$ menunjukkan suku ke-$i$ dalam lost function, yang sesuai dengan titik data ke-$i$. Bila kita menghilangkan superskrip demi kesederhanaan, setiap suku $\\mathcal{L}^{(i)}$ akan tampak seperti ini:\n",
        "\n",
        "$\\mathcal{L}^{(i)}=\\frac{1}{N}\\|xw+b - y\\|^2=\\frac{1}{N}\\|\\hat{y} - y\\|^2$\n",
        "\n",
        "Di sini notasi baru $\\hat{y}:= xw+b$ telah diperkenalkan untuk menunjukkan keluaran yang diprediksi oleh model regresi linier. Dengan notasi ini, hanya diperlukan kalkulus sederhana untuk menghitung turunan orde pertama:\n",
        "\n",
        "$\\frac{\\partial\\mathcal{L}^{(i)}}{\\partial w} = \\frac{2}{N}(\\hat{y} - y) x^\\top$\n",
        "\n",
        "$\\frac{\\partial\\mathcal{L}^{(i)}}{\\partial b} = \\frac{2}{N}(\\hat{y} - y)$\n",
        "\n",
        "Ingat bahwa turunan di atas adalah untuk suku ke-$i$ dalam lost function, yang hanya dikaitkan dengan titik data ke-$i$. Dengan kata lain, gradien global (gradien dari seluruh lost function) hanyalah penjumlahan dari gradien lokal (gradien suku ke-$i$), yang dapat dihitung secara independen dari titik data lainnya:\n",
        "\n",
        "$\\frac{\\partial\\mathcal{L}}{\\partial W} = \\frac{\\partial\\mathcal{L}^{(1)}}{\\partial W} + \\frac{\\partial\\mathcal{L}^{(2)}}{\\partial W} + \\cdots + \\frac{\\partial\\mathcal{L}^{(N)}}{\\partial W}$\n",
        "\n",
        "Ini berarti bahwa, tidak peduli bagaimana data didistribusikan, kita dapat menghitung gradien lokal ke-$i$ pada setiap titik data $(i)$ dan kemudian menggabungkannya untuk menghasilkan gradien global."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLk7MBNrAOaS"
      },
      "source": [
        "Untuk memahami konsepnya dengan lebih jelas, berikut ini contoh 10.000 titik data yang dibagi menjadi dua simpul komputasi (yang disimulasikan). Pertama, mari kita buat kumpulan data acak dengan $N=10.000$ dan $d=10%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "iDCSiHLjAOaS"
      },
      "outputs": [],
      "source": [
        "N = 10000\n",
        "d = 10\n",
        "X = np.random.normal(loc=0, scale=1, size=[N, d])\n",
        "Y = np.random.normal(loc=0, scale=1, size=[N, 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGNel9BrAOaS"
      },
      "source": [
        "Sekarang, mari kita bagi data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tC-rsNg6AOaS"
      },
      "outputs": [],
      "source": [
        "N_node1 = N//2\n",
        "N_node2 = N - N_node1\n",
        "\n",
        "X_node1 = X[:N_node1, :]\n",
        "Y_node1 = Y[:N_node1]\n",
        "X_node2 = X[N_node1:, :]\n",
        "Y_node2 = Y[N_node1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSFHzlssAOaS"
      },
      "source": [
        "Meskipun, kita tidak membagi kumpulan data secara fisik, kita beranggapan bahwa `X_node1` dan `Y_node1` hanya dapat diakses dari `node1` dan `X_node2` dan `Y_node2` hanya dapat diakses dari `node2`. Sekarang, pada node `master`, perintah berikut akan dijalankan:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vTNRcpgHAOaT"
      },
      "outputs": [],
      "source": [
        "# Initialize model parameters\n",
        "w = np.random.uniform(-1.0, 1.0, [d,1])\n",
        "b = np.random.uniform(-1.0, 1.0)\n",
        "\n",
        "MAX_ITER = 1000\n",
        "learning_rate = 0.01\n",
        "for i in range(MAX_ITER):\n",
        "    # Talk to node1 and ask it to compute the local gradient.\n",
        "    # (Pretend the following three lines are computed on node1)\n",
        "    err_node1 = np.matmul(X_node1, w) - Y_node1\n",
        "    dldw_node1 = np.mean(2*err_node1*X_node1, axis=0, keepdims=True)\n",
        "    dldb_node1 = np.mean(2*err_node1, axis=0, keepdims=True)\n",
        "\n",
        "    # Simultaneously, talk to node2 and ask the same.\n",
        "    # (Pretend the following three lines are computed on node2)\n",
        "    err_node2 = np.matmul(X_node2, w) - Y_node2\n",
        "    dldw_node2 = np.mean(2*err_node2*X_node2, axis=0, keepdims=True)\n",
        "    dldb_node2 = np.mean(2*err_node2, axis=0, keepdims=True)\n",
        "\n",
        "    # Aggregate the gradients by weighting them with the number of data available at each node.\n",
        "    dldw = (N_node1/N)*dldw_node1 + (N_node2/N)*dldw_node2\n",
        "    dldb = (N_node1/N)*dldb_node1 + (N_node2/N)*dldb_node2\n",
        "\n",
        "    # Update the model with the global gradient.\n",
        "    w -= learning_rate*dldw.T\n",
        "    b -= learning_rate*dldb.T\n",
        "\n",
        "    # If the solution does not improve much, break out of the for loop\n",
        "    if np.mean(dldw) < 1e-06:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbMJItsCAOaT"
      },
      "source": [
        "Tentu saja, hal di atas merupakan implementasi sederhana dari distributed gradient descent. Namun, dengan versi yang sederhana tersebut, kita dapat melihat hasilnya hampir sama dengan solusi pseudo-inverse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WM4VpoS-AOaT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bb3db46-cbc7-4750-8497-0d131e541dbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.04311594]\n",
            " [ 0.033538  ]\n",
            " [ 0.02315537]\n",
            " [-0.38982729]\n",
            " [-0.9304175 ]\n",
            " [ 0.6836212 ]\n",
            " [-0.04414589]\n",
            " [-0.45899713]\n",
            " [ 0.83113198]\n",
            " [-0.23302045]]\n",
            "[[ 5.02539438e-03]\n",
            " [-8.56898903e-03]\n",
            " [ 1.34404986e-02]\n",
            " [ 1.83127182e-05]\n",
            " [ 5.38381977e-03]\n",
            " [-1.57746743e-04]\n",
            " [-1.39371623e-02]\n",
            " [ 7.27290828e-03]\n",
            " [-6.97840401e-03]\n",
            " [-3.17077837e-03]]\n"
          ]
        }
      ],
      "source": [
        "XTX = np.linalg.inv(np.matmul(X.T, X))\n",
        "XTY = np.matmul(X.T, Y)\n",
        "w_true = np.matmul(XTX, XTY)\n",
        "print(w)   # distributed gradient descent solution\n",
        "print(w_true)  # pseudo-inverse solution (ground truth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUN4JCd1AOaT"
      },
      "source": [
        "Dengan contoh di atas, saya harap konsepnya kini sudah jelas di benak Anda. Bahkan jika Anda masih belum begitu yakin apakah Anda benar-benar dapat menulis kode seperti di atas dari awal, Anda seharusnya baik-baik saja, sejauh Anda memiliki gambaran besar yang jelas. Faktanya, penerapan pengoptimalan terdistribusi dan semacamnya ditangani oleh Spark. Sebaliknya, sebagai data scientist, Anda hanya perlu memiliki pemahaman dasar tentang cara kerjanya secara mendalam. Jadi, jangan khawatir."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuGMhULfAOaT"
      },
      "source": [
        "## Linear Regression in Spark\n",
        "\n",
        "Spark memiliki semua implementasi yang kuat dan teroptimasi dengan baik dari metode pengoptimalan terdistribusi tersebut (dan masih banyak lagi) di balik layar. Bahkan, dari sudut pandang pengguna, 99% dari waktu, Anda tidak perlu terlalu peduli dengan apa yang terjadi di balik layar. Spark akan memilih algoritme pengoptimalan yang paling sesuai untuk Anda dan melakukan semua pekerjaan berat di balik layar.\n",
        "\n",
        "Untuk melihat cara kerjanya, mari kita konfigurasikan Spark di Colab terlebih dahulu. (Jika Anda menjalankan notebook ini di komputer lokal dan telah mengonfigurasi Spark, Anda dapat melewati sel ini.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4QFQyTY-AOaT"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.3-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.3-bin-hadoop3\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ar2fFJtzAOaT"
      },
      "source": [
        "Sekarang, mari unduh kumpulan data untuk dicoba. Untuk tutorial ini, kita akan menggunakan [kumpulan data avocado](https://github.com/chainhaus/pythoncourse/raw/refs/heads/master/avocado.csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "IXO-ZrIbAOaT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52b19203-349c-4041-ae6d-5ac13dcf2557"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-25 11:31:12--  https://github.com/chainhaus/pythoncourse/raw/refs/heads/master/avocado.csv\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/chainhaus/pythoncourse/refs/heads/master/avocado.csv [following]\n",
            "--2024-10-25 11:31:12--  https://raw.githubusercontent.com/chainhaus/pythoncourse/refs/heads/master/avocado.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1989197 (1.9M) [text/plain]\n",
            "Saving to: ‘avocado.csv’\n",
            "\n",
            "avocado.csv         100%[===================>]   1.90M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-10-25 11:31:13 (143 MB/s) - ‘avocado.csv’ saved [1989197/1989197]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/chainhaus/pythoncourse/raw/refs/heads/master/avocado.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "734Ek5AjAOaT"
      },
      "source": [
        "Pada kuliah sebelumnya, kita telah mempelajari cara membaca file CSV sebagai Spark DataFrame. Di sini kita akan mengulang apa yang telah kita pelajari:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1WqClfXPAOaT"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('avocado').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-Vq4-rApAOaT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "302be999-ef15-4807-e935-dd5d00fa1ec5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+------------+------------+-------+---------+------+----------+----------+----------+-----------+------------+----+------+\n",
            "|_c0|      Date|AveragePrice|Total Volume|   4046|     4225|  4770|Total Bags|Small Bags|Large Bags|XLarge Bags|        type|year|region|\n",
            "+---+----------+------------+------------+-------+---------+------+----------+----------+----------+-----------+------------+----+------+\n",
            "|  0|2015-12-27|        1.33|    64236.62|1036.74| 54454.85| 48.16|   8696.87|   8603.62|     93.25|        0.0|conventional|2015|Albany|\n",
            "|  1|2015-12-20|        1.35|    54876.98| 674.28| 44638.81| 58.33|   9505.56|   9408.07|     97.49|        0.0|conventional|2015|Albany|\n",
            "|  2|2015-12-13|        0.93|   118220.22|  794.7|109149.67| 130.5|   8145.35|   8042.21|    103.14|        0.0|conventional|2015|Albany|\n",
            "|  3|2015-12-06|        1.08|    78992.15| 1132.0| 71976.41| 72.58|   5811.16|    5677.4|    133.76|        0.0|conventional|2015|Albany|\n",
            "|  4|2015-11-29|        1.28|     51039.6| 941.48| 43838.39| 75.78|   6183.95|   5986.26|    197.69|        0.0|conventional|2015|Albany|\n",
            "|  5|2015-11-22|        1.26|    55979.78|1184.27| 48067.99| 43.61|   6683.91|   6556.47|    127.44|        0.0|conventional|2015|Albany|\n",
            "|  6|2015-11-15|        0.99|    83453.76|1368.92| 73672.72| 93.26|   8318.86|   8196.81|    122.05|        0.0|conventional|2015|Albany|\n",
            "|  7|2015-11-08|        0.98|   109428.33| 703.75|101815.36|  80.0|   6829.22|   6266.85|    562.37|        0.0|conventional|2015|Albany|\n",
            "|  8|2015-11-01|        1.02|    99811.42|1022.15| 87315.57| 85.34|  11388.36|  11104.53|    283.83|        0.0|conventional|2015|Albany|\n",
            "|  9|2015-10-25|        1.07|    74338.76|  842.4| 64757.44| 113.0|   8625.92|   8061.47|    564.45|        0.0|conventional|2015|Albany|\n",
            "| 10|2015-10-18|        1.12|    84843.44| 924.86| 75595.85|117.07|   8205.66|   7877.86|     327.8|        0.0|conventional|2015|Albany|\n",
            "| 11|2015-10-11|        1.28|    64489.17|1582.03| 52677.92|105.32|   10123.9|   9866.27|    257.63|        0.0|conventional|2015|Albany|\n",
            "| 12|2015-10-04|        1.31|     61007.1|2268.32| 49880.67|101.36|   8756.75|   8379.98|    376.77|        0.0|conventional|2015|Albany|\n",
            "| 13|2015-09-27|        0.99|   106803.39|1204.88| 99409.21|154.84|   6034.46|   5888.87|    145.59|        0.0|conventional|2015|Albany|\n",
            "| 14|2015-09-20|        1.33|    69759.01|1028.03| 59313.12| 150.5|   9267.36|    8489.1|    778.26|        0.0|conventional|2015|Albany|\n",
            "| 15|2015-09-13|        1.28|    76111.27| 985.73| 65696.86| 142.0|   9286.68|   8665.19|    621.49|        0.0|conventional|2015|Albany|\n",
            "| 16|2015-09-06|        1.11|    99172.96| 879.45| 90062.62|240.79|    7990.1|   7762.87|    227.23|        0.0|conventional|2015|Albany|\n",
            "| 17|2015-08-30|        1.07|   105693.84| 689.01| 94362.67|335.43|  10306.73|  10218.93|      87.8|        0.0|conventional|2015|Albany|\n",
            "| 18|2015-08-23|        1.34|    79992.09| 733.16| 67933.79|444.78|  10880.36|  10745.79|    134.57|        0.0|conventional|2015|Albany|\n",
            "| 19|2015-08-16|        1.33|    80043.78| 539.65| 68666.01| 394.9|  10443.22|  10297.68|    145.54|        0.0|conventional|2015|Albany|\n",
            "+---+----------+------------+------------+-------+---------+------+----------+----------+----------+-----------+------------+----+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.read.csv('avocado.csv', header=True, sep=',', inferSchema=True)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "divvz6xnAOaT"
      },
      "source": [
        "Sekarang, untuk menggunakan library Spark ML, Anda harus terlebih dahulu mengonversi kolom menjadi vektor fitur. Untuk kumpulan data ini, kita diharapkan dapat memprediksi harga avocado (kolom ke-dua) menggunakan fitur-fitur seperti Total, Volume, dll. (semua kolom lainnya).\n",
        "\n",
        "Untuk tujuan ini, Spark menyediakan metode praktis yang disebut `VectorAssembler` untuk menghasilkan vektor fitur dengan merakit kolom-kolom DataFrame.\n",
        "\n",
        "Namun perlu dilihat bahwa `VectorAssemble` hanya untuk data numerk. Oleh sebab itu kolom non numerik dihilangkan dulu. Dan juga kolom yang tidak dibutuhkan lainnya."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop('_c0', 'Date', 'type', 'year', 'region')"
      ],
      "metadata": {
        "id": "zprwhnKxiQ_r"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "k7TdltQzAOaT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65d764d1-e747-4b8e-e2d6-d34ddbbeb578"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+------------+-------+---------+------+----------+----------+----------+-----------+--------------------+\n",
            "|AveragePrice|Total Volume|   4046|     4225|  4770|Total Bags|Small Bags|Large Bags|XLarge Bags|            features|\n",
            "+------------+------------+-------+---------+------+----------+----------+----------+-----------+--------------------+\n",
            "|        1.33|    64236.62|1036.74| 54454.85| 48.16|   8696.87|   8603.62|     93.25|        0.0|[1.33,64236.62,10...|\n",
            "|        1.35|    54876.98| 674.28| 44638.81| 58.33|   9505.56|   9408.07|     97.49|        0.0|[1.35,54876.98,67...|\n",
            "|        0.93|   118220.22|  794.7|109149.67| 130.5|   8145.35|   8042.21|    103.14|        0.0|[0.93,118220.22,7...|\n",
            "|        1.08|    78992.15| 1132.0| 71976.41| 72.58|   5811.16|    5677.4|    133.76|        0.0|[1.08,78992.15,11...|\n",
            "|        1.28|     51039.6| 941.48| 43838.39| 75.78|   6183.95|   5986.26|    197.69|        0.0|[1.28,51039.6,941...|\n",
            "|        1.26|    55979.78|1184.27| 48067.99| 43.61|   6683.91|   6556.47|    127.44|        0.0|[1.26,55979.78,11...|\n",
            "|        0.99|    83453.76|1368.92| 73672.72| 93.26|   8318.86|   8196.81|    122.05|        0.0|[0.99,83453.76,13...|\n",
            "|        0.98|   109428.33| 703.75|101815.36|  80.0|   6829.22|   6266.85|    562.37|        0.0|[0.98,109428.33,7...|\n",
            "|        1.02|    99811.42|1022.15| 87315.57| 85.34|  11388.36|  11104.53|    283.83|        0.0|[1.02,99811.42,10...|\n",
            "|        1.07|    74338.76|  842.4| 64757.44| 113.0|   8625.92|   8061.47|    564.45|        0.0|[1.07,74338.76,84...|\n",
            "|        1.12|    84843.44| 924.86| 75595.85|117.07|   8205.66|   7877.86|     327.8|        0.0|[1.12,84843.44,92...|\n",
            "|        1.28|    64489.17|1582.03| 52677.92|105.32|   10123.9|   9866.27|    257.63|        0.0|[1.28,64489.17,15...|\n",
            "|        1.31|     61007.1|2268.32| 49880.67|101.36|   8756.75|   8379.98|    376.77|        0.0|[1.31,61007.1,226...|\n",
            "|        0.99|   106803.39|1204.88| 99409.21|154.84|   6034.46|   5888.87|    145.59|        0.0|[0.99,106803.39,1...|\n",
            "|        1.33|    69759.01|1028.03| 59313.12| 150.5|   9267.36|    8489.1|    778.26|        0.0|[1.33,69759.01,10...|\n",
            "|        1.28|    76111.27| 985.73| 65696.86| 142.0|   9286.68|   8665.19|    621.49|        0.0|[1.28,76111.27,98...|\n",
            "|        1.11|    99172.96| 879.45| 90062.62|240.79|    7990.1|   7762.87|    227.23|        0.0|[1.11,99172.96,87...|\n",
            "|        1.07|   105693.84| 689.01| 94362.67|335.43|  10306.73|  10218.93|      87.8|        0.0|[1.07,105693.84,6...|\n",
            "|        1.34|    79992.09| 733.16| 67933.79|444.78|  10880.36|  10745.79|    134.57|        0.0|[1.34,79992.09,73...|\n",
            "|        1.33|    80043.78| 539.65| 68666.01| 394.9|  10443.22|  10297.68|    145.54|        0.0|[1.33,80043.78,53...|\n",
            "+------------+------------+-------+---------+------+----------+----------+----------+-----------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "# Creates a new column called 'features' that contains feature vectors.\n",
        "assembler = VectorAssembler(inputCols=df.columns[:-1], outputCol=\"features\")\n",
        "df_vec = assembler.transform(df)\n",
        "df_vec.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcFR-tyMAOaU"
      },
      "source": [
        "Sekarang setelah kita membuat vektor fitur, mari kita bagi set data menjadi dua, yaitu set pelatihan dan set pengujian. Spark DataFrame menawarkan metode siap pakai untuk melakukannya:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "gxR1D1kvAOaU"
      },
      "outputs": [],
      "source": [
        "df_train, df_test = df_vec.randomSplit([0.7, 0.3])  # 70% of the original data will be used for training and 30% for testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_plg3ieEAOaU"
      },
      "source": [
        "Sekarang, bagian training sebenarnya cukup sederhana seperti dijelaskan di bawah ini."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Ld91X_8tAOaU"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "lr = LinearRegression(featuresCol='features', labelCol='AveragePrice')\n",
        "lr_model = lr.fit(df_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA82_dcUAOaU"
      },
      "source": [
        "Setelah proses training selesai, hasilnya dapat ditemukan dengan memanggil anggota model seperti `lr_model.coefficients` atau `lr_model.intercept`. Untuk detail selengkapnya, lihat https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.regression.LinearRegressionModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "CEasw3rvAOaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d5c1879-6a5a-46c6-c6db-f270a0f29ed4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.0000000000000016,-1.2554165511188418e-19,1.2572195484961194e-19,1.2545376381167065e-19,1.2622890124481839e-19,1.2343246348503944e-19,2.0537545694172307e-21,2.360631011498478e-21]\n",
            "-2.54729474236351e-15\n"
          ]
        }
      ],
      "source": [
        "print( lr_model.coefficients )  # slope of the linear equation\n",
        "print( lr_model.intercept )     # intercept of the line"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJhIRDAoAOaU"
      },
      "source": [
        "Selain itu, Anda juga dapat melihat ringkasan hasil training dengan memanggil fungsi (`lr_model.summary`). Untuk selanjutnya, daftar fungsi-fungsi tersedia di [URL ini](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.regression.LinearRegressionModel)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "fWWKEEPTAOaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6030e819-e6d3-47e9-ed59-4827974ff565"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[residuals: double]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "lr_model.summary.residuals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "83uboBUjAOaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb016a60-6124-432d-ff96-3f008732b838"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.65761461752283e-16"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "lr_model.summary.rootMeanSquaredError\n",
        "# lr_model.summary.meanAbsoluteError\n",
        "# lr_model.summary.meanSquaredError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "YzcXYLszAOaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b2c2a35-e70f-40a0-d106-e1182c7c5afe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.499209506198322e-17,\n",
              " 1.0274675039389723e-19,\n",
              " 1.0274621900191644e-19,\n",
              " 1.0274634085446786e-19,\n",
              " 1.0274810931516787e-19,\n",
              " 1.0272043725145008e-19,\n",
              " 6.458056880127409e-22,\n",
              " 6.212280658393967e-22,\n",
              " 2.2194688393483494e-17]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "lr_model.summary.coefficientStandardErrors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "0BEC_W_2AOaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7999a795-60e5-4e49-b4e6-a2ffd4e90c38"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "lr_model.summary.r2\n",
        "lr_model.summary.r2adj\n",
        "# lr_model.summary.pValues"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyOZbBotAOaU"
      },
      "source": [
        "Model yang ditraining menggunakan Spark dapat diuji dengan menggunakan metode `evaluate()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Yc0FZEdxAOaU"
      },
      "outputs": [],
      "source": [
        "evaluation_summary = lr_model.evaluate(df_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTCMXcQEAOaU"
      },
      "source": [
        "Evaluation summary object pada dasarnya sama dengan model summary object. Dengan kata lain, apa yang telah Anda lihat di atas berlaku untuk ringkasan evaluasi:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "sqqopmXwAOaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88d03a2c-2fa4-41c7-cb3f-c5c1f2430d8b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.660947188737428e-16"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "evaluation_summary.rootMeanSquaredError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "vu7IBmoNAOaV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ec4f647-0297-4b9c-8745-0330b0619d22"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "lr_model.summary.totalIterations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation summary di atas pada dasarnya adalah metrics untuk mengukur kinerja regresi. Ini bisa Anda implementasikan di kasus regresi atau perhitungan estimasi lainnya. Seperti kalian lihar, nilai RMSE `Root Mean Square Error` sangat rendah, berarti keberhasilan prediksi sangat tinggi.\n",
        "\n",
        "\n",
        "Sekarang, kalian pasti penasaran, bagaimana hasil prediksinya. Jalankan perintah berikut:"
      ],
      "metadata": {
        "id": "DH6GDMT0tT5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = lr_model.transform(df_test)\n",
        "predictions.show()"
      ],
      "metadata": {
        "id": "Cu1QZbVDsOob",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8abaa885-0c30-4b9c-b99a-15b771fd464b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+------------+----------+---------+--------+----------+----------+----------+-----------+--------------------+-------------------+\n",
            "|AveragePrice|Total Volume|      4046|     4225|    4770|Total Bags|Small Bags|Large Bags|XLarge Bags|            features|         prediction|\n",
            "+------------+------------+----------+---------+--------+----------+----------+----------+-----------+--------------------+-------------------+\n",
            "|        0.46|  2200550.27|1200632.86|531226.65|18324.93| 450365.83| 113752.17|  330583.1|    6030.56|[0.46,2200550.27,...|0.45999999999999847|\n",
            "|        0.49|  1137707.43|  738314.8|286858.37|11642.46|  100891.8|  70749.02|  30142.78|        0.0|[0.49,1137707.43,...| 0.4899999999999983|\n",
            "|        0.51|    41987.86|    225.44|  5734.39|     0.0|  36028.03|    473.98|  35554.05|        0.0|[0.51,41987.86,22...| 0.5099999999999982|\n",
            "|        0.51|  1442973.47|1037699.01|259846.68| 14567.4| 130860.38|   76814.4|  54045.98|        0.0|[0.51,1442973.47,...| 0.5099999999999986|\n",
            "|        0.52|  1457359.83|1130917.54|199669.94| 4499.84| 122272.51|  90030.35|  32242.16|        0.0|[0.52,1457359.83,...| 0.5199999999999984|\n",
            "|        0.53|  1097224.25| 785254.94| 204147.3|10346.68|  97475.33|  72169.92|  25305.41|        0.0|[0.53,1097224.25,...| 0.5299999999999985|\n",
            "|        0.53|  1272428.72|1012900.04|159158.29| 5832.62|  94537.77|  76637.06|  17900.71|        0.0|[0.53,1272428.72,...| 0.5299999999999984|\n",
            "|        0.53|  1547337.25| 1067091.8|116819.32|42393.74| 321032.39| 292106.96|  28925.43|        0.0|[0.53,1547337.25,...| 0.5299999999999985|\n",
            "|        0.53|  5470227.08|1741607.02|937331.61|89678.63|2701609.82|2656630.42|    3465.7|    41513.7|[0.53,5470227.08,...| 0.5299999999999983|\n",
            "|        0.54|   369757.62| 191441.47| 57532.08|  594.02| 120190.05|  53302.46|  66879.47|       8.12|[0.54,369757.62,1...| 0.5399999999999985|\n",
            "|        0.54|  1343180.92|1028045.58|188115.35| 4910.71| 122109.28|  99129.04|  22980.24|        0.0|[0.54,1343180.92,...| 0.5399999999999986|\n",
            "|        0.54|  1601222.68| 909748.65|296612.15|15561.81| 379300.07|   98982.8| 280317.27|        0.0|[0.54,1601222.68,...| 0.5399999999999986|\n",
            "|        0.56|     10571.3|       0.0|  1277.78|     0.0|   9293.52|    287.77|   9005.75|        0.0|[0.56,10571.3,0.0...| 0.5599999999999984|\n",
            "|        0.56|   822548.66| 385584.08|189389.79|  105.28| 247469.51|  98976.22| 148493.29|        0.0|[0.56,822548.66,3...| 0.5599999999999984|\n",
            "|        0.56|  1105500.34| 760680.02|271207.14| 13354.8|  60258.38|  60255.64|      2.74|        0.0|[0.56,1105500.34,...| 0.5599999999999985|\n",
            "|        0.56|  1456835.79|1096452.67|220124.25| 5251.74| 135007.13|  98977.04|  36030.09|        0.0|[0.56,1456835.79,...| 0.5599999999999986|\n",
            "|        0.56|  2120511.03| 837700.97|162473.38| 4190.48| 1116146.2|  952254.7| 161188.17|    2703.33|[0.56,2120511.03,...| 0.5599999999999985|\n",
            "|        0.57|  1290021.19| 816554.74|311746.69|22261.01| 139458.75|   69825.0|  69633.75|        0.0|[0.57,1290021.19,...| 0.5699999999999984|\n",
            "|        0.57|  1417208.02| 933181.79|198434.84| 2690.98| 282900.41| 190479.48|  92420.93|        0.0|[0.57,1417208.02,...| 0.5699999999999984|\n",
            "|        0.58|   542750.89| 348348.45| 55886.98|   566.5| 137948.96| 114075.87|  23873.09|        0.0|[0.58,542750.89,3...| 0.5799999999999985|\n",
            "+------------+------------+----------+---------+--------+----------+----------+----------+-----------+--------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLEKP3AwAOaV"
      },
      "source": [
        "## Transformers and Estimators\n",
        "\n",
        "Saya harap sekarang Anda sudah lebih memahami cara kerja Spark ML. Nah, dalam latihan di atas, ada banyak detail. Namun, ada beberapa konsep penting yang perlu Anda kuasai, agar benar-benar dapat memanfaatkan Spark ML."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn34K0DxAOaV"
      },
      "source": [
        "Kita buka load lagi file avocado.csv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "3KpGmd71AOaV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b296a85-81db-4135-db44-fed5ee8b471f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+------------+------------+-------+---------+------+----------+----------+----------+-----------+------------+----+------+\n",
            "|_c0|      Date|AveragePrice|Total Volume|   4046|     4225|  4770|Total Bags|Small Bags|Large Bags|XLarge Bags|        type|year|region|\n",
            "+---+----------+------------+------------+-------+---------+------+----------+----------+----------+-----------+------------+----+------+\n",
            "|  0|2015-12-27|        1.33|    64236.62|1036.74| 54454.85| 48.16|   8696.87|   8603.62|     93.25|        0.0|conventional|2015|Albany|\n",
            "|  1|2015-12-20|        1.35|    54876.98| 674.28| 44638.81| 58.33|   9505.56|   9408.07|     97.49|        0.0|conventional|2015|Albany|\n",
            "|  2|2015-12-13|        0.93|   118220.22|  794.7|109149.67| 130.5|   8145.35|   8042.21|    103.14|        0.0|conventional|2015|Albany|\n",
            "|  3|2015-12-06|        1.08|    78992.15| 1132.0| 71976.41| 72.58|   5811.16|    5677.4|    133.76|        0.0|conventional|2015|Albany|\n",
            "|  4|2015-11-29|        1.28|     51039.6| 941.48| 43838.39| 75.78|   6183.95|   5986.26|    197.69|        0.0|conventional|2015|Albany|\n",
            "|  5|2015-11-22|        1.26|    55979.78|1184.27| 48067.99| 43.61|   6683.91|   6556.47|    127.44|        0.0|conventional|2015|Albany|\n",
            "|  6|2015-11-15|        0.99|    83453.76|1368.92| 73672.72| 93.26|   8318.86|   8196.81|    122.05|        0.0|conventional|2015|Albany|\n",
            "|  7|2015-11-08|        0.98|   109428.33| 703.75|101815.36|  80.0|   6829.22|   6266.85|    562.37|        0.0|conventional|2015|Albany|\n",
            "|  8|2015-11-01|        1.02|    99811.42|1022.15| 87315.57| 85.34|  11388.36|  11104.53|    283.83|        0.0|conventional|2015|Albany|\n",
            "|  9|2015-10-25|        1.07|    74338.76|  842.4| 64757.44| 113.0|   8625.92|   8061.47|    564.45|        0.0|conventional|2015|Albany|\n",
            "| 10|2015-10-18|        1.12|    84843.44| 924.86| 75595.85|117.07|   8205.66|   7877.86|     327.8|        0.0|conventional|2015|Albany|\n",
            "| 11|2015-10-11|        1.28|    64489.17|1582.03| 52677.92|105.32|   10123.9|   9866.27|    257.63|        0.0|conventional|2015|Albany|\n",
            "| 12|2015-10-04|        1.31|     61007.1|2268.32| 49880.67|101.36|   8756.75|   8379.98|    376.77|        0.0|conventional|2015|Albany|\n",
            "| 13|2015-09-27|        0.99|   106803.39|1204.88| 99409.21|154.84|   6034.46|   5888.87|    145.59|        0.0|conventional|2015|Albany|\n",
            "| 14|2015-09-20|        1.33|    69759.01|1028.03| 59313.12| 150.5|   9267.36|    8489.1|    778.26|        0.0|conventional|2015|Albany|\n",
            "| 15|2015-09-13|        1.28|    76111.27| 985.73| 65696.86| 142.0|   9286.68|   8665.19|    621.49|        0.0|conventional|2015|Albany|\n",
            "| 16|2015-09-06|        1.11|    99172.96| 879.45| 90062.62|240.79|    7990.1|   7762.87|    227.23|        0.0|conventional|2015|Albany|\n",
            "| 17|2015-08-30|        1.07|   105693.84| 689.01| 94362.67|335.43|  10306.73|  10218.93|      87.8|        0.0|conventional|2015|Albany|\n",
            "| 18|2015-08-23|        1.34|    79992.09| 733.16| 67933.79|444.78|  10880.36|  10745.79|    134.57|        0.0|conventional|2015|Albany|\n",
            "| 19|2015-08-16|        1.33|    80043.78| 539.65| 68666.01| 394.9|  10443.22|  10297.68|    145.54|        0.0|conventional|2015|Albany|\n",
            "+---+----------+------------+------------+-------+---------+------+----------+----------+----------+-----------+------------+----+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.read.csv('avocado.csv', header=True, sep=',', inferSchema=True)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-k7p6DBvAOaV"
      },
      "source": [
        "Nah, berikut topik utama hari ini. Yang baru saja kita buat adalah DataFrame yang berisi kolom dengan variabel kategoris. Kolom category dalam bentuk string, jadi kita perlu mengonversinya menjadi nilai numerik. Ini adalah tugas sehari-hari yang cukup umum bagi ilmuwan data. (Ya, tentu saja, kita bisa saja membuat kolom `type` dengan indeks numerik di tempat pertama, tetapi kita sedang mensimulasikan situasi dunia nyata di sini.)\n",
        "\n",
        "Di spark, ada beberapa helper method untuk melakukannya, yakni `StringIndexer` dan `OneHotEncoder`. StringIndexer secara harfiah adalah metode untuk mengonversi variabel kategoris tipe string menjadi indeks numerik. OneHotEncoder, di sisi lain, adalah metode lain yang mengonversi indeks numerik menjadi [one-hot encoder](https://en.wikipedia.org/wiki/One-hot). Ini adalah fitur yang cukup sering digunakan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "anLLENCbAOaV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c1b2b96-38d9-4a79-93fa-c00dab53f518"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+------------+------------+-------+---------+------+----------+----------+----------+-----------+------------+----+------+----------+\n",
            "|_c0|      Date|AveragePrice|Total Volume|   4046|     4225|  4770|Total Bags|Small Bags|Large Bags|XLarge Bags|        type|year|region|type_index|\n",
            "+---+----------+------------+------------+-------+---------+------+----------+----------+----------+-----------+------------+----+------+----------+\n",
            "|  0|2015-12-27|        1.33|    64236.62|1036.74| 54454.85| 48.16|   8696.87|   8603.62|     93.25|        0.0|conventional|2015|Albany|       0.0|\n",
            "|  1|2015-12-20|        1.35|    54876.98| 674.28| 44638.81| 58.33|   9505.56|   9408.07|     97.49|        0.0|conventional|2015|Albany|       0.0|\n",
            "|  2|2015-12-13|        0.93|   118220.22|  794.7|109149.67| 130.5|   8145.35|   8042.21|    103.14|        0.0|conventional|2015|Albany|       0.0|\n",
            "|  3|2015-12-06|        1.08|    78992.15| 1132.0| 71976.41| 72.58|   5811.16|    5677.4|    133.76|        0.0|conventional|2015|Albany|       0.0|\n",
            "|  4|2015-11-29|        1.28|     51039.6| 941.48| 43838.39| 75.78|   6183.95|   5986.26|    197.69|        0.0|conventional|2015|Albany|       0.0|\n",
            "|  5|2015-11-22|        1.26|    55979.78|1184.27| 48067.99| 43.61|   6683.91|   6556.47|    127.44|        0.0|conventional|2015|Albany|       0.0|\n",
            "|  6|2015-11-15|        0.99|    83453.76|1368.92| 73672.72| 93.26|   8318.86|   8196.81|    122.05|        0.0|conventional|2015|Albany|       0.0|\n",
            "|  7|2015-11-08|        0.98|   109428.33| 703.75|101815.36|  80.0|   6829.22|   6266.85|    562.37|        0.0|conventional|2015|Albany|       0.0|\n",
            "|  8|2015-11-01|        1.02|    99811.42|1022.15| 87315.57| 85.34|  11388.36|  11104.53|    283.83|        0.0|conventional|2015|Albany|       0.0|\n",
            "|  9|2015-10-25|        1.07|    74338.76|  842.4| 64757.44| 113.0|   8625.92|   8061.47|    564.45|        0.0|conventional|2015|Albany|       0.0|\n",
            "| 10|2015-10-18|        1.12|    84843.44| 924.86| 75595.85|117.07|   8205.66|   7877.86|     327.8|        0.0|conventional|2015|Albany|       0.0|\n",
            "| 11|2015-10-11|        1.28|    64489.17|1582.03| 52677.92|105.32|   10123.9|   9866.27|    257.63|        0.0|conventional|2015|Albany|       0.0|\n",
            "| 12|2015-10-04|        1.31|     61007.1|2268.32| 49880.67|101.36|   8756.75|   8379.98|    376.77|        0.0|conventional|2015|Albany|       0.0|\n",
            "| 13|2015-09-27|        0.99|   106803.39|1204.88| 99409.21|154.84|   6034.46|   5888.87|    145.59|        0.0|conventional|2015|Albany|       0.0|\n",
            "| 14|2015-09-20|        1.33|    69759.01|1028.03| 59313.12| 150.5|   9267.36|    8489.1|    778.26|        0.0|conventional|2015|Albany|       0.0|\n",
            "| 15|2015-09-13|        1.28|    76111.27| 985.73| 65696.86| 142.0|   9286.68|   8665.19|    621.49|        0.0|conventional|2015|Albany|       0.0|\n",
            "| 16|2015-09-06|        1.11|    99172.96| 879.45| 90062.62|240.79|    7990.1|   7762.87|    227.23|        0.0|conventional|2015|Albany|       0.0|\n",
            "| 17|2015-08-30|        1.07|   105693.84| 689.01| 94362.67|335.43|  10306.73|  10218.93|      87.8|        0.0|conventional|2015|Albany|       0.0|\n",
            "| 18|2015-08-23|        1.34|    79992.09| 733.16| 67933.79|444.78|  10880.36|  10745.79|    134.57|        0.0|conventional|2015|Albany|       0.0|\n",
            "| 19|2015-08-16|        1.33|    80043.78| 539.65| 68666.01| 394.9|  10443.22|  10297.68|    145.54|        0.0|conventional|2015|Albany|       0.0|\n",
            "+---+----------+------------+------------+-------+---------+------+----------+----------+----------+-----------+------------+----+------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "indexer = StringIndexer(inputCol = 'type', outputCol = 'type_index')\n",
        "df = indexer.fit(df).transform(df)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "VvWQ4N03AOaV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a279c30-db47-4bd0-b2c6-13e74edbdf9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+------------+------------+-------+---------+------+----------+----------+----------+-----------+------------+----+------+----------+-------------+\n",
            "|_c0|      Date|AveragePrice|Total Volume|   4046|     4225|  4770|Total Bags|Small Bags|Large Bags|XLarge Bags|        type|year|region|type_index|   type_dummy|\n",
            "+---+----------+------------+------------+-------+---------+------+----------+----------+----------+-----------+------------+----+------+----------+-------------+\n",
            "|  0|2015-12-27|        1.33|    64236.62|1036.74| 54454.85| 48.16|   8696.87|   8603.62|     93.25|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "|  1|2015-12-20|        1.35|    54876.98| 674.28| 44638.81| 58.33|   9505.56|   9408.07|     97.49|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "|  2|2015-12-13|        0.93|   118220.22|  794.7|109149.67| 130.5|   8145.35|   8042.21|    103.14|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "|  3|2015-12-06|        1.08|    78992.15| 1132.0| 71976.41| 72.58|   5811.16|    5677.4|    133.76|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "|  4|2015-11-29|        1.28|     51039.6| 941.48| 43838.39| 75.78|   6183.95|   5986.26|    197.69|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "|  5|2015-11-22|        1.26|    55979.78|1184.27| 48067.99| 43.61|   6683.91|   6556.47|    127.44|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "|  6|2015-11-15|        0.99|    83453.76|1368.92| 73672.72| 93.26|   8318.86|   8196.81|    122.05|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "|  7|2015-11-08|        0.98|   109428.33| 703.75|101815.36|  80.0|   6829.22|   6266.85|    562.37|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "|  8|2015-11-01|        1.02|    99811.42|1022.15| 87315.57| 85.34|  11388.36|  11104.53|    283.83|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "|  9|2015-10-25|        1.07|    74338.76|  842.4| 64757.44| 113.0|   8625.92|   8061.47|    564.45|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "| 10|2015-10-18|        1.12|    84843.44| 924.86| 75595.85|117.07|   8205.66|   7877.86|     327.8|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "| 11|2015-10-11|        1.28|    64489.17|1582.03| 52677.92|105.32|   10123.9|   9866.27|    257.63|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "| 12|2015-10-04|        1.31|     61007.1|2268.32| 49880.67|101.36|   8756.75|   8379.98|    376.77|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "| 13|2015-09-27|        0.99|   106803.39|1204.88| 99409.21|154.84|   6034.46|   5888.87|    145.59|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "| 14|2015-09-20|        1.33|    69759.01|1028.03| 59313.12| 150.5|   9267.36|    8489.1|    778.26|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "| 15|2015-09-13|        1.28|    76111.27| 985.73| 65696.86| 142.0|   9286.68|   8665.19|    621.49|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "| 16|2015-09-06|        1.11|    99172.96| 879.45| 90062.62|240.79|    7990.1|   7762.87|    227.23|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "| 17|2015-08-30|        1.07|   105693.84| 689.01| 94362.67|335.43|  10306.73|  10218.93|      87.8|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "| 18|2015-08-23|        1.34|    79992.09| 733.16| 67933.79|444.78|  10880.36|  10745.79|    134.57|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "| 19|2015-08-16|        1.33|    80043.78| 539.65| 68666.01| 394.9|  10443.22|  10297.68|    145.54|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "+---+----------+------------+------------+-------+---------+------+----------+----------+----------+-----------+------------+----+------+----------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import OneHotEncoder\n",
        "encoder = OneHotEncoder(inputCols=['type_index'], outputCols=['type_dummy'])\n",
        "df = encoder.fit(df).transform(df)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jybJ3AX0AOaV"
      },
      "source": [
        "Sekarang, pada contoh di atas, kami menggunakan metode `fit()` dan `transform()` pada DataFrame `df`. Kedua fungsi ini, pada kenyataannya, terkait dengan dua konsep inti yang ingin kami pelajari dalam sesi ini---yaitu **transformer** dan **estimator**.\n",
        "\n",
        "Menurut [Dokumentasi Resmi Spark](https://spark.apache.org/docs/latest/ml-pipeline.html), **transformer** adalah abstraksi yang mencakup transformer fitur dan model yang dipelajari. Secara sederhana, transformer adalah objek yang dilengkapi dengan logika yang dapat mengubah satu DataFrame menjadi yang lain. Misalnya, transformer dapat mengambil DataFrame, membaca kolom (misalnya variabel kategoris), memetakannya ke kolom baru (misalnya indeks numerik), dan mengeluarkan DataFrame baru dengan kolom yang dipetakan ditambahkan di bagian akhir. Untuk contoh lain, transformer juga dapat mengambil DataFrame, membaca vektor fitur dalam kolom, memprediksi label untuk setiap vektor fitur, dan mengeluarkan DataFrame baru dengan label yang diprediksi. Oleh karena itu, model pembelajaran mesin (yang dilatih) adalah transformer, dalam artian yang mengubah DataFrame input menjadi DataFrame output.\n",
        "\n",
        "Di sisi lain, **estimator** mengabstraksikan konsep algoritma pembelajaran atau algoritma apa pun yang sesuai atau dilatih pada data. Estimator secara harfiah mengambil DataFrame dan *memperkirakan* parameter dengan menggunakan nilai dalam DataFrame. Misalnya, `LinearRegression` yang kita lihat di atas adalah estimator dan memanggil `fit()` untuk melatih `LinearRegressionModel`, yang merupakan output dari `fit()`. Output `LinearRegressionModel` di sisi lain adalah transformer, yang dapat digunakan untuk mengubah DataFrame uji untuk menghasilkan label yang diprediksi.\n",
        "\n",
        "Cara mudah untuk membedakan transformer dan estimator adalah dengan melihat metode yang dimilikinya. Secara teknis, transformer mengimplementasikan metode `transform()`, yang mengubah satu DataFrame menjadi yang lain, umumnya dengan menambahkan satu atau beberapa kolom. Di sisi lain, estimator mengimplementasikan metode `fit()`, yang mengambil DataFrame dan menghasilkan, biasanya transformer.\n",
        "\n",
        "Dengan semua ini, mari kita uraikan sel-sel di atas. Pertama-tama, `StringIndexer` adalah estimator yang mengambil DataFrame dan menghasilkan transformer. Bagaimana saya mengetahuinya? Yah, ia dilengkapi dengan fungsi `fit()`, jadi saya dapat dengan aman berasumsi bahwa ia adalah estimator. Lebih khusus lagi, estimator StringIndexer memperkirakan jumlah kategori yang berbeda dari DataFrame dan menghasilkan transformer yang mengimplementasikan logika untuk memetakan setiap kategori yang berbeda menjadi nilai numerik. Jumlah kategori berbeda dalam DataFrame bervariasi dari satu masalah ke masalah lainnya, jadi Anda perlu memperkirakan parameter tersebut (jumlah kategori) terlebih dahulu, sebelum dapat mengubah DataFrame.\n",
        "\n",
        "Output dari `StringIndexer.fit()`, seperti yang disebutkan di atas, adalah sebuah transformator. Ia berisi logika untuk memetakan setiap kategori string ke dalam indeks numerik. Memanggil `transform()` akan menerapkan logika tersebut dan akan mengubah DataFrame input menjadi DataFrame output.\n",
        "\n",
        "Apakah sintaks `indexer.fit(df).transform(df)` masuk akal sekarang? Anda dapat menginterpretasikan kode `OneHotEncoder` dengan cara yang sama. Mengetahui bahwa Anda semua adalah orang pintar, saya tidak akan menjelaskan secara eksplisit apa itu baris demi baris."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4nI2cyUAOaV"
      },
      "source": [
        "Baiklah, sekarang Anda baru saja menguasai salah satu gagasan terpenting dalam Spark. Sebagai catatan kecil, Anda akan melihat bahwa satu representasi hot encoded dalam kolom `type_dummy` terlihat agak aneh:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.select('type').distinct().show()"
      ],
      "metadata": {
        "id": "2M_l6T19kzu9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aab12d0-6788-4fe2-8297-1090854c9a3f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|        type|\n",
            "+------------+\n",
            "|     organic|\n",
            "|conventional|\n",
            "+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "NY1E8t83AOaV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "261ed16f-1b17-46bf-8061-51ea908091e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+------------+------------+-----+------+----+----------+----------+----------+-----------+-------+----+------+----------+----------+\n",
            "|_c0|      Date|AveragePrice|Total Volume| 4046|  4225|4770|Total Bags|Small Bags|Large Bags|XLarge Bags|   type|year|region|type_index|type_dummy|\n",
            "+---+----------+------------+------------+-----+------+----+----------+----------+----------+-----------+-------+----+------+----------+----------+\n",
            "|  0|2015-12-27|        1.83|      989.55| 8.16| 88.59| 0.0|     892.8|     892.8|       0.0|        0.0|organic|2015|Albany|       1.0| (1,[],[])|\n",
            "|  1|2015-12-20|        1.89|     1163.03|30.24|172.14| 0.0|    960.65|    960.65|       0.0|        0.0|organic|2015|Albany|       1.0| (1,[],[])|\n",
            "|  2|2015-12-13|        1.85|      995.96|10.44| 178.7| 0.0|    806.82|    806.82|       0.0|        0.0|organic|2015|Albany|       1.0| (1,[],[])|\n",
            "|  3|2015-12-06|        1.84|     1158.42|90.29|104.18| 0.0|    963.95|    948.52|     15.43|        0.0|organic|2015|Albany|       1.0| (1,[],[])|\n",
            "|  4|2015-11-29|        1.94|      831.69|  0.0| 94.73| 0.0|    736.96|    736.96|       0.0|        0.0|organic|2015|Albany|       1.0| (1,[],[])|\n",
            "|  5|2015-11-22|        1.94|      858.83|13.84| 84.18| 0.0|    760.81|    755.69|      5.12|        0.0|organic|2015|Albany|       1.0| (1,[],[])|\n",
            "|  6|2015-11-15|        1.89|     1208.54|20.71|238.16| 0.0|    949.67|    949.67|       0.0|        0.0|organic|2015|Albany|       1.0| (1,[],[])|\n",
            "|  7|2015-11-08|        1.88|     1332.27|20.08| 351.4| 0.0|    960.79|    960.79|       0.0|        0.0|organic|2015|Albany|       1.0| (1,[],[])|\n",
            "|  8|2015-11-01|        1.88|     1021.68|11.47|137.58| 0.0|    872.63|    872.63|       0.0|        0.0|organic|2015|Albany|       1.0| (1,[],[])|\n",
            "|  9|2015-10-25|        1.83|      1161.9|49.27|148.96| 0.0|    963.67|    963.67|       0.0|        0.0|organic|2015|Albany|       1.0| (1,[],[])|\n",
            "| 10|2015-10-18|        1.97|      969.29|10.31|158.07| 0.0|    800.91|    800.91|       0.0|        0.0|organic|2015|Albany|       1.0| (1,[],[])|\n",
            "| 11|2015-10-11|         1.9|     1170.01|28.65| 88.25| 0.0|   1053.11|   1035.28|     17.83|        0.0|organic|2015|Albany|       1.0| (1,[],[])|\n",
            "| 12|2015-10-04|        1.98|     1145.88| 5.74|165.26| 0.0|    974.88|    964.68|      10.2|        0.0|organic|2015|Albany|       1.0| (1,[],[])|\n",
            "| 13|2015-09-27|        1.98|      814.13|13.79|140.23| 0.0|    660.11|    660.11|       0.0|        0.0|organic|2015|Albany|       1.0| (1,[],[])|\n",
            "| 14|2015-09-20|        1.98|       774.2|42.63|228.13| 0.0|    503.44|    503.44|       0.0|        0.0|organic|2015|Albany|       1.0| (1,[],[])|\n",
            "| 15|2015-09-13|        1.99|       902.5|13.86|105.13| 0.0|    783.51|    783.51|       0.0|        0.0|organic|2015|Albany|       1.0| (1,[],[])|\n",
            "| 16|2015-09-06|        1.86|     1168.86|30.13| 96.18| 0.0|   1042.55|   1042.55|       0.0|        0.0|organic|2015|Albany|       1.0| (1,[],[])|\n",
            "| 17|2015-08-30|        1.88|     1239.16|17.27| 65.08| 0.0|   1156.81|   1156.81|       0.0|        0.0|organic|2015|Albany|       1.0| (1,[],[])|\n",
            "| 18|2015-08-23|        1.87|     1275.64|24.45| 97.82| 0.0|   1153.37|   1153.37|       0.0|        0.0|organic|2015|Albany|       1.0| (1,[],[])|\n",
            "| 19|2015-08-16|         2.0|     1467.23|24.56|168.41| 0.0|   1274.26|   1274.26|       0.0|        0.0|organic|2015|Albany|       1.0| (1,[],[])|\n",
            "+---+----------+------------+------------+-----+------+----+----------+----------+----------+-----------+-------+----+------+----------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.where(df['type']=='organic').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTqBoBTWAOaV"
      },
      "source": [
        "Seperti yang dapat Anda lihat di atas, nilai-nilai dalam kolom `type_dummy` tampak seperti `(1,[],[])`, dan Anda mungkin bertanya-tanya, apa...? Nah, ini sebenarnya adalah representasi vektor sparse yang digunakan oleh Spark. Jika Anda pikirkan tentang hal itu, one hot encoding adalah cara yang cukup tidak efisien untuk merepresentasikan sebuah kelas. Dalam kasus kita, kita hanya memiliki dua kelas, yaitu `conventional` dan `organic`. Namun dalam banyak kasus, Anda akan sering melihat ratusan atau ribuan kategori. Dalam kasus tersebut, one-hot encoding akan berakhir tampak seperti vektor seribu dimensi (vektor dengan seribu entri) yang semuanya diisi dengan '0' kecuali untuk satu entri. Jadi, menyimpan nol yang tidak perlu dalam DataFrame adalah pemborosan memori yang sangat besar, terutama jika Anda berbicara tentang masalah big data. Atas dasar ini, Spark menggunakan representasi sparse di mana, alih-alih menyimpan semua entri dengan nol, ia hanya menyimpan elemen yang bukan nol. Untuk melakukannya, Anda memerlukan tiga hal. Pertama, Anda perlu mengetahui dimensi vektor, karena Anda akan melewatkan banyak entri. Kedua, Anda perlu mengetahui posisi nilai bukan nol, karena alasan yang jelas. Terakhir, Anda perlu mengetahui apa sebenarnya nilai-nilai tersebut.\n",
        "\n",
        "Ketiga hal ini, pada kenyataannya, adalah apa yang disimpan Spark untuk mewakili vektor enkode one-hot. Angka pertama dalam tuple `(1,[],[])` menunjukkan dimensi vektor enkode one-hot, yang dalam kasus ini adalah 1 (= jumlah kategori - 1). Objek kedua dalam tuple adalah daftar posisi elemen bukan nol, yang dalam kasus ini adalah array kosong---artinya tidak ada elemen bukan nol. Terakhir, objek ketiga adalah daftar nilai sebenarnya dari elemen bukan nol, yang sekali lagi, dalam kasus kita adalah array kosong karena kita tidak memiliki elemen bukan nol. Jadi, rekonstruksi representasi sparse `(1,[],[])` akan menjadi `[0]` dalam representasi padat---vektor satu dimensi dengan elemen '0'.\n",
        "\n",
        "Perhatikan bahwa ada dua `type` avocado lainnya, yakni `conventional`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "yi15ed_5AOaV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6948f66b-bcd4-4751-f14e-083166ddee0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+------------+------------+-------+---------+------+----------+----------+----------+-----------+------------+----+------+----------+-------------+\n",
            "|_c0|      Date|AveragePrice|Total Volume|   4046|     4225|  4770|Total Bags|Small Bags|Large Bags|XLarge Bags|        type|year|region|type_index|   type_dummy|\n",
            "+---+----------+------------+------------+-------+---------+------+----------+----------+----------+-----------+------------+----+------+----------+-------------+\n",
            "|  0|2015-12-27|        1.33|    64236.62|1036.74| 54454.85| 48.16|   8696.87|   8603.62|     93.25|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "|  1|2015-12-20|        1.35|    54876.98| 674.28| 44638.81| 58.33|   9505.56|   9408.07|     97.49|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "|  2|2015-12-13|        0.93|   118220.22|  794.7|109149.67| 130.5|   8145.35|   8042.21|    103.14|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "|  3|2015-12-06|        1.08|    78992.15| 1132.0| 71976.41| 72.58|   5811.16|    5677.4|    133.76|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "|  4|2015-11-29|        1.28|     51039.6| 941.48| 43838.39| 75.78|   6183.95|   5986.26|    197.69|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "|  5|2015-11-22|        1.26|    55979.78|1184.27| 48067.99| 43.61|   6683.91|   6556.47|    127.44|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "|  6|2015-11-15|        0.99|    83453.76|1368.92| 73672.72| 93.26|   8318.86|   8196.81|    122.05|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "|  7|2015-11-08|        0.98|   109428.33| 703.75|101815.36|  80.0|   6829.22|   6266.85|    562.37|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "|  8|2015-11-01|        1.02|    99811.42|1022.15| 87315.57| 85.34|  11388.36|  11104.53|    283.83|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "|  9|2015-10-25|        1.07|    74338.76|  842.4| 64757.44| 113.0|   8625.92|   8061.47|    564.45|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "| 10|2015-10-18|        1.12|    84843.44| 924.86| 75595.85|117.07|   8205.66|   7877.86|     327.8|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "| 11|2015-10-11|        1.28|    64489.17|1582.03| 52677.92|105.32|   10123.9|   9866.27|    257.63|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "| 12|2015-10-04|        1.31|     61007.1|2268.32| 49880.67|101.36|   8756.75|   8379.98|    376.77|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "| 13|2015-09-27|        0.99|   106803.39|1204.88| 99409.21|154.84|   6034.46|   5888.87|    145.59|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "| 14|2015-09-20|        1.33|    69759.01|1028.03| 59313.12| 150.5|   9267.36|    8489.1|    778.26|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "| 15|2015-09-13|        1.28|    76111.27| 985.73| 65696.86| 142.0|   9286.68|   8665.19|    621.49|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "| 16|2015-09-06|        1.11|    99172.96| 879.45| 90062.62|240.79|    7990.1|   7762.87|    227.23|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "| 17|2015-08-30|        1.07|   105693.84| 689.01| 94362.67|335.43|  10306.73|  10218.93|      87.8|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "| 18|2015-08-23|        1.34|    79992.09| 733.16| 67933.79|444.78|  10880.36|  10745.79|    134.57|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "| 19|2015-08-16|        1.33|    80043.78| 539.65| 68666.01| 394.9|  10443.22|  10297.68|    145.54|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|\n",
            "+---+----------+------------+------------+-------+---------+------+----------+----------+----------+-----------+------------+----+------+----------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.where(df['type']=='conventional').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_l6J9ERAOaW"
      },
      "source": [
        "Apakah Anda memperhatikan bahwa sekarang kita memiliki `(1,[0],[1.0])` alih-alih `(1,[],[])`? Sekali lagi, interpretasinya sama: kita memiliki vektor satu dimensi (`1`); posisi elemen bukan nol berada pada posisi ke-0 (`[0]`); nilai elemen bukan nol adalah 1.0 (`[1.0]`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qivz6awRAOaW"
      },
      "source": [
        "Oke, lanjut. Sebagai langkah berikutnya, kita perlu memvektorisasi kolom untuk melatih model regresi linier. Sebenarnya, kita sudah melakukannya di atas, jadi kita cukup mendaur ulang kode sebelumnya. Berhati-hatilah karena kita akhirnya menambahkan beberapa kolom lagi sebagai hasil dari StringIndexer dan OneHotEncoder. Namun terlebih dahulu hilangkan komom non numerik lainnya"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop('Date', '_c0', 'type', 'year', 'region')"
      ],
      "metadata": {
        "id": "i_oYFG1Gnz23"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "NA8YSX--AOaW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21e107cc-7503-4c6b-8f13-9665748ec200"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------------+\n",
            "|            features|AveragePrice|\n",
            "+--------------------+------------+\n",
            "|[1.33,64236.62,10...|        1.33|\n",
            "|[1.35,54876.98,67...|        1.35|\n",
            "|[0.93,118220.22,7...|        0.93|\n",
            "|[1.08,78992.15,11...|        1.08|\n",
            "|[1.28,51039.6,941...|        1.28|\n",
            "|[1.26,55979.78,11...|        1.26|\n",
            "|[0.99,83453.76,13...|        0.99|\n",
            "|[0.98,109428.33,7...|        0.98|\n",
            "|[1.02,99811.42,10...|        1.02|\n",
            "|[1.07,74338.76,84...|        1.07|\n",
            "|[1.12,84843.44,92...|        1.12|\n",
            "|[1.28,64489.17,15...|        1.28|\n",
            "|[1.31,61007.1,226...|        1.31|\n",
            "|[0.99,106803.39,1...|        0.99|\n",
            "|[1.33,69759.01,10...|        1.33|\n",
            "|[1.28,76111.27,98...|        1.28|\n",
            "|[1.11,99172.96,87...|        1.11|\n",
            "|[1.07,105693.84,6...|        1.07|\n",
            "|[1.34,79992.09,73...|        1.34|\n",
            "|[1.33,80043.78,53...|        1.33|\n",
            "+--------------------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "assembler = VectorAssembler(inputCols=df.columns[:-4] + ['type_dummy'], outputCol=\"features\")\n",
        "df_vec = assembler.transform(df).select(['features', 'AveragePrice'])\n",
        "df_vec.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5P8lr9cAOaW"
      },
      "source": [
        "Nak, sekarang, apakah Anda menyadari bahwa `VectorAssembler` adalah sebuah transformator? Bagus. Saya yakin Anda menyadarinya. Apakah Anda juga menyadari bahwa sementara `StringIndexer` dan `OneHotEncoder` adalah estimator, `VectorAssembler` adalah sebuah transformator? Dapatkah Anda menebak mengapa demikian? Jelaskan!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjD4B6ogAOaW"
      },
      "source": [
        "Terakhir, kita dapat melatih model regresi linier dengan kolom `type_dummy` tambahan sebagai regresor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "mdlfzMGLAOaW"
      },
      "outputs": [],
      "source": [
        "df_train, df_test = df_vec.randomSplit([0.7, 0.3])\n",
        "\n",
        "lr = LinearRegression(featuresCol='features', labelCol='AveragePrice')\n",
        "lr_model = lr.fit(df_train)\n",
        "\n",
        "predicted = lr_model.evaluate(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted.rootMeanSquaredError"
      ],
      "metadata": {
        "id": "vMIO5eghuRIT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58be2cb2-7d66-4a4b-f34f-55b6a16a6955"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = lr_model.transform(df_test)\n",
        "predictions.show()"
      ],
      "metadata": {
        "id": "iI5PUwP-udy4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e493be07-1447-491e-957f-6f08be59b910"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------------+----------+\n",
            "|            features|AveragePrice|prediction|\n",
            "+--------------------+------------+----------+\n",
            "|(8,[0,1,2,3],[1.4...|        1.41|      1.41|\n",
            "|(8,[0,1,2,3],[1.8...|        1.83|      1.83|\n",
            "|(8,[0,1,2,3],[2.3...|        2.36|      2.36|\n",
            "|(8,[0,1,2,3],[2.7...|        2.79|      2.79|\n",
            "|(8,[0,1,3,5],[1.8...|        1.87|      1.87|\n",
            "|[0.51,17135.45,48...|        0.51|      0.51|\n",
            "|[0.51,41987.86,22...|        0.51|      0.51|\n",
            "|[0.51,1366844.88,...|        0.51|      0.51|\n",
            "|[0.53,1097224.25,...|        0.53|      0.53|\n",
            "|[0.53,1272428.72,...|        0.53|      0.53|\n",
            "|[0.53,1613159.67,...|        0.53|      0.53|\n",
            "|[0.54,1423939.62,...|        0.54|      0.54|\n",
            "|[0.54,1582877.09,...|        0.54|      0.54|\n",
            "|[0.55,1609195.36,...|        0.55|      0.55|\n",
            "|[0.55,1977923.65,...|        0.55|      0.55|\n",
            "|[0.56,9801.7,9.25...|        0.56|      0.56|\n",
            "|[0.56,10571.3,0.0...|        0.56|      0.56|\n",
            "|[0.56,1236204.18,...|        0.56|      0.56|\n",
            "|[0.56,1339528.26,...|        0.56|      0.56|\n",
            "|[0.56,1387970.0,9...|        0.56|      0.56|\n",
            "+--------------------+------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRjQIk87AOaW"
      },
      "source": [
        "Sekali lagi, apakah Anda memperhatikan bahwa `LinearRegression` memiliki metode `fit()` dan dengan demikian merupakan estimator? Dan objek `LinearRegressionModel`, yang dalam kasus kita disimpan dalam variabel `lr_model`, merupakan transformer?\n",
        "\n",
        "Jadi, seperti yang Anda lihat, transformer dan estimator merupakan konsep inti dalam Spark. Faktanya, banyak contoh yang akan kita lihat nanti dalam kursus ini pada dasarnya hanyalah estimator dan transformer yang berbeda."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FyETNvEAOaW"
      },
      "source": [
        "## Pipeline\n",
        "Satu hal terakhir: seperti yang kita lihat di kelas DataFrame, Spark dibangun di atas konsep 'lazy execution', artinya, hingga pengguna meminta nilai numerik yang sebenarnya, tidak ada penghitungan angka yang dilakukan. Sebaliknya, Spark terus membangun grafik komputasi, yang menggambarkan logika tentang bagaimana angka akan dihitung. Pada akhirnya, hanya ketika pengguna meminta hasil numerik yang sebenarnya dengan memanggil, katakanlah `DataFrame.show()`, Spark benar-benar mengalirkan angka melalui grafik dan melakukan penghitungan angka yang sebenarnya. Kita melihat ini karena masalah efisiensi. Prinsip yang sama diterapkan pada transformer dan estimator. Dengan kata lain, estimator dan transformer hanya membangun grafik komputasi. Hanya ketika pengguna memanggil misalnya `DataFrame.show()`, grafik komputasi benar-benar dieksekusi dan angka mengalir melalui grafik.\n",
        "\n",
        "Dalam beberapa hal, grafik komputasi seperti 'pipe'/'pipa', tempat data mengalir. Sebagai seorang arsitek, Anda menambahkan pipa dengan bentuk yang berbeda ke dalam pipa. Hanya saat Anda siap mengalirkan air, Anda membuka katup dan membiarkan air mengalir. Dengan menggunakan analogi yang sama, di Spark, Anda dapat (dan harus) mendefinisikan sebuah jaringan pipa. Lebih khusus lagi, Spark Pipeline adalah objek yang mencakup bagaimana transformer dan estimator (=pipa) dihubungkan. Saat pengguna mencari hasil numerik, Spark menjalankan angka dalam DataFrame (=air) melalui Pipeline dan menghasilkan sebuah hasil.\n",
        "\n",
        "Untuk membangun sebuah Pipeline, kita harus mendefinisikan tahapan:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "AtoOn013AOaW"
      },
      "outputs": [],
      "source": [
        "stages = []  # create an empty list\n",
        "\n",
        "indexer = StringIndexer(inputCol = 'type', outputCol = 'type_index')\n",
        "encoder = OneHotEncoder(inputCols=['type_index'], outputCols=['type_dummy'])\n",
        "assembler = VectorAssembler(inputCols=df.columns[:-4] + ['type_dummy'], outputCol=\"features\")\n",
        "\n",
        "stages += [indexer, encoder, assembler]  # append the estimators and transformers (ordering matters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYtTy5PiAOaW"
      },
      "source": [
        "Sekarang, menyiapkan jalur pipa sebenarnya hanya memerlukan beberapa baris kode:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "PRH3t6EKAOaW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be6a6c90-62eb-40b5-93fd-3d9c56e01426"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+------------+------------+-------+---------+------+----------+----------+----------+-----------+------------+----+------+----------+-------------+--------------------+\n",
            "|_c0|      Date|AveragePrice|Total Volume|   4046|     4225|  4770|Total Bags|Small Bags|Large Bags|XLarge Bags|        type|year|region|type_index|   type_dummy|            features|\n",
            "+---+----------+------------+------------+-------+---------+------+----------+----------+----------+-----------+------------+----+------+----------+-------------+--------------------+\n",
            "|  0|2015-12-27|        1.33|    64236.62|1036.74| 54454.85| 48.16|   8696.87|   8603.62|     93.25|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|[1.33,64236.62,10...|\n",
            "|  1|2015-12-20|        1.35|    54876.98| 674.28| 44638.81| 58.33|   9505.56|   9408.07|     97.49|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|[1.35,54876.98,67...|\n",
            "|  2|2015-12-13|        0.93|   118220.22|  794.7|109149.67| 130.5|   8145.35|   8042.21|    103.14|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|[0.93,118220.22,7...|\n",
            "|  3|2015-12-06|        1.08|    78992.15| 1132.0| 71976.41| 72.58|   5811.16|    5677.4|    133.76|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|[1.08,78992.15,11...|\n",
            "|  4|2015-11-29|        1.28|     51039.6| 941.48| 43838.39| 75.78|   6183.95|   5986.26|    197.69|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|[1.28,51039.6,941...|\n",
            "|  5|2015-11-22|        1.26|    55979.78|1184.27| 48067.99| 43.61|   6683.91|   6556.47|    127.44|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|[1.26,55979.78,11...|\n",
            "|  6|2015-11-15|        0.99|    83453.76|1368.92| 73672.72| 93.26|   8318.86|   8196.81|    122.05|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|[0.99,83453.76,13...|\n",
            "|  7|2015-11-08|        0.98|   109428.33| 703.75|101815.36|  80.0|   6829.22|   6266.85|    562.37|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|[0.98,109428.33,7...|\n",
            "|  8|2015-11-01|        1.02|    99811.42|1022.15| 87315.57| 85.34|  11388.36|  11104.53|    283.83|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|[1.02,99811.42,10...|\n",
            "|  9|2015-10-25|        1.07|    74338.76|  842.4| 64757.44| 113.0|   8625.92|   8061.47|    564.45|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|[1.07,74338.76,84...|\n",
            "| 10|2015-10-18|        1.12|    84843.44| 924.86| 75595.85|117.07|   8205.66|   7877.86|     327.8|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|[1.12,84843.44,92...|\n",
            "| 11|2015-10-11|        1.28|    64489.17|1582.03| 52677.92|105.32|   10123.9|   9866.27|    257.63|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|[1.28,64489.17,15...|\n",
            "| 12|2015-10-04|        1.31|     61007.1|2268.32| 49880.67|101.36|   8756.75|   8379.98|    376.77|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|[1.31,61007.1,226...|\n",
            "| 13|2015-09-27|        0.99|   106803.39|1204.88| 99409.21|154.84|   6034.46|   5888.87|    145.59|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|[0.99,106803.39,1...|\n",
            "| 14|2015-09-20|        1.33|    69759.01|1028.03| 59313.12| 150.5|   9267.36|    8489.1|    778.26|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|[1.33,69759.01,10...|\n",
            "| 15|2015-09-13|        1.28|    76111.27| 985.73| 65696.86| 142.0|   9286.68|   8665.19|    621.49|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|[1.28,76111.27,98...|\n",
            "| 16|2015-09-06|        1.11|    99172.96| 879.45| 90062.62|240.79|    7990.1|   7762.87|    227.23|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|[1.11,99172.96,87...|\n",
            "| 17|2015-08-30|        1.07|   105693.84| 689.01| 94362.67|335.43|  10306.73|  10218.93|      87.8|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|[1.07,105693.84,6...|\n",
            "| 18|2015-08-23|        1.34|    79992.09| 733.16| 67933.79|444.78|  10880.36|  10745.79|    134.57|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|[1.34,79992.09,73...|\n",
            "| 19|2015-08-16|        1.33|    80043.78| 539.65| 68666.01| 394.9|  10443.22|  10297.68|    145.54|        0.0|conventional|2015|Albany|       0.0|(1,[0],[1.0])|[1.33,80043.78,53...|\n",
            "+---+----------+------------+------------+-------+---------+------+----------+----------+----------+-----------+------------+----+------+----------+-------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.read.csv('avocado.csv', header=True, sep=',', inferSchema=True) # overwrite df by recreating it.\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "pipeline = Pipeline(stages = stages)\n",
        "pipelineModel = pipeline.fit(df)\n",
        "df_vec = pipelineModel.transform(df)\n",
        "df_vec.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4Fw8vqVAOaW"
      },
      "source": [
        "Sekali lagi, perhatikan bahwa Pipeline itu sendiri adalah estimator, yang dapat `fit()` ke DataFrame untuk menghasilkan transformer. Hasil dari transformer adalah DataFrame baru yang dikonversi dari DataFrame input dengan menerapkan urutan `indexer`--`encoder`--`assembler`. Output dari pipeline harus sama persis dengan yang dari bagian sebelumnya.\n",
        "\n",
        "Jika outputnya sama, mengapa kita repot-repot menggunakan Pipeline? Pertama, penulisan kode program lebih sederhana, seperti yang mungkin telah Anda perhatikan. Ini  memiliki efek optimalisasi komputasi, karena menjadi lebih jelas bagaimana transformer dan estimator harus dihubungkan.\n",
        "\n",
        "Untuk informasi lebih lanjut, Anda sangat dianjurkan untuk mempelajari dokumentasi resmi Spark: https://spark.apache.org/docs/latest/ml-pipeline.html"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tugas\n",
        "Buat model regresi menggunakan pipeline, dengan spesifikasi:\n",
        "- gunakan kolom `region` ubah sebagai one-hot-encoder\n",
        "- bagi data menjadi df_train dan df_test dengan proporsi 0/7 dan 0.3\n",
        "- buat model regresinya menggunakan df_train\n",
        "- uji model dengan df_test dan tampilkan RMSE\n",
        "- tampilkan hasil prediksi terhadap df_test\n",
        "- diskusikan apakah hasil prediksi menggunakan `region` hasilnya sama atau berbeda dibandingkan `type`"
      ],
      "metadata": {
        "id": "FaijZd0IppLa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encode"
      ],
      "metadata": {
        "id": "76EUszHfVRyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Kolom region\n",
        "region_indexer = StringIndexer(inputCol=\"region\", outputCol=\"region_index\")\n",
        "region_encoder = OneHotEncoder(inputCol=\"region_index\", outputCol=\"region_encoded\")\n",
        "\n",
        "# 2. Assembling feature columns\n",
        "assembler = VectorAssembler(inputCols=[\"region_encoded\", \"AveragePrice\", \"Total Volume\", \"4046\", \"4225\", \"4770\", \"Total Bags\", \"Small Bags\", \"Large Bags\", \"XLarge Bags\"], outputCol=\"features\")\n",
        "\n",
        "# 3. Membuat model regresi\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=\"AveragePrice\")\n",
        "\n",
        "# 4. Membuat pipeline\n",
        "stages = [region_indexer, region_encoder, assembler, lr]\n",
        "pipeline = Pipeline(stages=stages)"
      ],
      "metadata": {
        "id": "D9LWB73xpoPV"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spliting Data"
      ],
      "metadata": {
        "id": "JjdF0d_QVdGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_test = df.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "# 5. Melatih model regresi menggunakan df_train\n",
        "pipeline_model = pipeline.fit(df_train)"
      ],
      "metadata": {
        "id": "U03aGNJGVheU"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modeling"
      ],
      "metadata": {
        "id": "lpoGFeraVj8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_predictions = pipeline_model.transform(df_test)"
      ],
      "metadata": {
        "id": "11qyRx5qVl8P"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "evaluator = RegressionEvaluator(labelCol=\"AveragePrice\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(df_predictions)\n",
        "print(f\"Root Mean Squared Error (RMSE) on test data = {rmse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFKPEgtWV4T5",
        "outputId": "73fd4edd-18e6-4758-be29-3c46e363a123"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared Error (RMSE) on test data = 1.3810964163543961e-16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_predictions.select(\"prediction\", \"AveragePrice\", \"region\").show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSx8MCWNWB8q",
        "outputId": "48155337-1cca-4a80-8e55-d226e3c09ebb"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+------------+----------------+\n",
            "|        prediction|AveragePrice|          region|\n",
            "+------------------+------------+----------------+\n",
            "|              0.78|        0.78|         Houston|\n",
            "|0.8299999999999998|        0.83|        LasVegas|\n",
            "|              0.87|        0.87|CincinnatiDayton|\n",
            "|0.9000000000000002|         0.9|      California|\n",
            "|0.9299999999999999|        0.93|         Chicago|\n",
            "|0.9499999999999998|        0.95|         TotalUS|\n",
            "|0.9599999999999999|        0.96|         Roanoke|\n",
            "|0.9699999999999999|        0.97|           Boise|\n",
            "|0.9799999999999999|        0.98| RichmondNorfolk|\n",
            "|0.9799999999999999|        0.98|      Sacramento|\n",
            "+------------------+------------+----------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tulis deskripsi dan analisa kalian di sini\n",
        "\n",
        "Model berhasil menangkap hubungan antara fitur dan variabel target di berbagai wilayah, seperti yang ditunjukkan oleh keselarasan yang tepat antara harga prediksi dan harga aktual di kota-kota seperti Houston, Las Vegas, dan Sacramento. Meskipun model berkinerja sangat baik, kesalahan yang rendah tersebut dapat mengindikasikan potensi overfitting, dan validasi atau investigasi lebih lanjut terhadap kumpulan data mungkin diperlukan untuk memastikan generalisasi yang kuat pada data yang tidak terlihat."
      ],
      "metadata": {
        "id": "SUFD2n9FvChd"
      }
    }
  ],
  "metadata": {
    "@webio": {
      "lastCommId": null,
      "lastKernelId": null
    },
    "kernelspec": {
      "display_name": "Python [conda env:bigdata]",
      "language": "python",
      "name": "conda-env-bigdata-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}